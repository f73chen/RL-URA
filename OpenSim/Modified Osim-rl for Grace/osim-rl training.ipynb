{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'stable_baselines'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-8d64491d665c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mstable_baselines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicies\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMlpPolicy\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mstable_baselines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvec_env\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSubprocVecEnv\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mstable_baselines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommon\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mset_global_seeds\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'stable_baselines'"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import bz2\n",
    "import shutil\n",
    "import pickle\n",
    "from IPython.display import clear_output\n",
    "from osim.env.osimMod36d import L2RunEnvMod\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", category=DeprecationWarning)\n",
    "    import tensorflow as tf\n",
    "\n",
    "from stable_baselines.common.policies import MlpPolicy\n",
    "from stable_baselines.common.vec_env import SubprocVecEnv\n",
    "from stable_baselines.common import set_global_seeds\n",
    "from stable_baselines import PPO2\n",
    "from stable_baselines.bench import Monitor\n",
    "from stable_baselines.results_plotter import load_results, ts2xy\n",
    "from stable_baselines.common.callbacks import BaseCallback, EveryNTimesteps\n",
    "\n",
    "############################################################################## Set up Directories\n",
    "log_dir = 'log/'\n",
    "tensorboard_log_dir = log_dir+\"tensorboard/\"\n",
    "os.makedirs(log_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(env_in, rank, time_limit, log_dir, seed=0, stepsize=0.01, **kwargs):\n",
    "    \"\"\"\n",
    "    Utility function for multiprocessed env.\n",
    "    \n",
    "    :param env_id: (str) the environment ID\n",
    "    :param num_env: (int) the number of environment you wish to have in subprocesses \n",
    "    :param seed: (int) the inital seed for RNG\n",
    "    :param rank: (int) index of the subprocess\n",
    "    \"\"\"\n",
    "    if os.path.exists(log_dir + '/env_0/monitor.csv'):\n",
    "        raise Exception(\"existing monitor files found!!!\")\n",
    "    \n",
    "    def _init():\n",
    "        env_in.time_limit = time_limit\n",
    "        env = env_in(**kwargs) \n",
    "        env.osim_model.stepsize = stepsize\n",
    "        log_sub_dir = log_dir + '/env_{}'.format(str(rank))\n",
    "        os.makedirs(log_sub_dir, exist_ok=True)\n",
    "        env = Monitor(env, log_sub_dir, allow_early_resets=True)\n",
    "        env.seed(seed + rank)\n",
    "        return env\n",
    "    set_global_seeds(seed)\n",
    "    return _init\n",
    "\n",
    "def learning_rate(frac):\n",
    "    # 3.0e-4*(np.exp(n*(frac-1))), used n=2 for 10m steps\n",
    "    return params['lr_a1']*(np.exp(params['lr_a2']*(frac-1)))\n",
    "\n",
    "def own_policy(obs):\n",
    "    action = np.zeros(18)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_xy(log_dir, num_rollout):\n",
    "    y = []\n",
    "    for folder in os.listdir(log_dir):\n",
    "        if folder.startswith('env_'):\n",
    "            _, y_tmp = ts2xy(load_results(log_dir+folder), 'timesteps')\n",
    "            if len(y_tmp) > 0:\n",
    "                y.extend(list(y_tmp[-num_rollout:]))\n",
    "    y = sum(y)/len(y) if len(y) > 0 else -np.inf \n",
    "    return y\n",
    "\n",
    "class LogCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    A custom callback that derives from ``BaseCallback``.\n",
    "\n",
    "    :param verbose: (int) Verbosity level 0: not output 1: info 2: debug\n",
    "    \"\"\"\n",
    "    def __init__(self, log_dir, verbose=0, num_rollout=5):\n",
    "        self.log_dir = log_dir\n",
    "        self.best_mean_reward = -np.inf\n",
    "        self.num_rollout = num_rollout\n",
    "        super(LogCallback, self).__init__(verbose)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        mean_reward = extract_xy(self.log_dir, self.num_rollout)\n",
    "        if mean_reward != -np.inf:\n",
    "            clear_output(wait=True)\n",
    "            print(self.num_timesteps, 'timesteps')\n",
    "            print(\"Best mean reward: {:.2f} - Last mean reward per episode: {:.2f}\". \\\n",
    "                  format(self.best_mean_reward, mean_reward))\n",
    "\n",
    "            # New best model, you could save the agent here\n",
    "            if mean_reward > self.best_mean_reward:\n",
    "                self.best_mean_reward = mean_reward\n",
    "                # Example for saving best model\n",
    "                print(\"Saving new best model\")\n",
    "                self.model.save(self.log_dir + 'best_model')\n",
    "                self.model.save(self.log_dir + 'latest_model')\n",
    "            else:\n",
    "                print(\"Saving latest model\")\n",
    "                self.model.save(self.log_dir + 'latest_model')\n",
    "        return True\n",
    "    \n",
    "log_callback = LogCallback(log_dir, num_rollout=5)\n",
    "event_callback = EveryNTimesteps(n_steps=2000, callback=log_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params = {'reward_weight': [6.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.0, 0.5],\n",
    "          #['forward', 'survival', 'torso', 'joint', 'stability', 'act', 'footstep', 'jerk', 'slide']\n",
    "          'action_limit': [1]*18,\n",
    "          'time_limit': 1000,\n",
    "          'stepsize': 0.01,\n",
    "          'integrator_accuracy': 5e-3,\n",
    "          'seed': 0,\n",
    "          'num_cpu': 12,\n",
    "          'lr_a1': 1.0e-4,\n",
    "          'lr_a2': 2, \n",
    "          'target_speed_range': [0.8,1.2],\n",
    "          'total_timesteps': 10000000}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Environment and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SubprocVecEnv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-870a806a9b0a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Create the vectorized environment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# env_in, rank, time_limit, log_dir, seed=0, stepsize=0.01, env_related arguments\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m env = SubprocVecEnv([make_env(L2RunEnvMod, i, params['time_limit'], log_dir, \n\u001b[0m\u001b[0;32m      4\u001b[0m                               \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'seed'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                               \u001b[0mstepsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'stepsize'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'SubprocVecEnv' is not defined"
     ]
    }
   ],
   "source": [
    "# Create the vectorized environment\n",
    "# env_in, rank, time_limit, log_dir, seed=0, stepsize=0.01, env_related arguments\n",
    "env = SubprocVecEnv([make_env(L2RunEnvMod, i, params['time_limit'], log_dir, \n",
    "                              seed=params['seed'], \n",
    "                              stepsize=params['stepsize'], \n",
    "                              reward_weight = params['reward_weight'], \n",
    "                              action_limit = params['action_limit'], \n",
    "                              visualize=False,\n",
    "                              integrator_accuracy=params['integrator_accuracy'], \n",
    "                              target_speed_range = params['target_speed_range'], \n",
    "                              own_policy=own_policy) \n",
    "                     for i in range(params['num_cpu'])])\n",
    "\n",
    "# define policy network\n",
    "policy_kwargs = dict(act_fun=tf.nn.tanh, net_arch=[dict(vf=[256,256,256], pi=[256,256,256,12])])\n",
    "\n",
    "# batch_size=n_steps*num_cpu\n",
    "model = PPO2(MlpPolicy, env, verbose=0, policy_kwargs=policy_kwargs, \n",
    "             noptepochs=4, n_steps=128, learning_rate=learning_rate, \n",
    "             tensorboard_log=tensorboard_log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this block if want to load parameters from a pretrained model\n",
    "model.load_parameters(log_dir+'latest_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "965928 timesteps\n",
      "Best mean reward: 4.52 - Last mean reward per episode: -0.38\n",
      "Saving latest model\n"
     ]
    }
   ],
   "source": [
    "# log environment and training parameters before start training\n",
    "params_path = log_dir+'params.pbz2'\n",
    "if os.path.exists(params_path):\n",
    "    raise Exception(\"existing env_learning_params file found!\")\n",
    "else:\n",
    "    with bz2.BZ2File(params_path, 'w') as f: \n",
    "        pickle.dump(params, f)\n",
    "\n",
    "# lbackup custom osim environment to log-dir before start training\n",
    "shutil.copy2('osim/env/osimMod36d.py', log_dir+'osimMod36d.py')\n",
    "\n",
    "model.learn(total_timesteps=params['total_timesteps'], callback=event_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Reward Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(values, window):\n",
    "    \"\"\"\n",
    "    Smooth values by doing a moving average\n",
    "    :param values: (numpy array)\n",
    "    :param window: (int)\n",
    "    :return: (numpy array)\n",
    "    \"\"\"\n",
    "    weights = np.repeat(1.0, window) / window\n",
    "    return np.convolve(values, weights, 'valid')\n",
    "\n",
    "\n",
    "def plot_results(log_folder, title='Learning Curve', instances=1, same_plot=False):\n",
    "    \"\"\"\n",
    "    plot the results\n",
    "\n",
    "    :param log_folder: (str) the save location of the results to plot\n",
    "    :param title: (str) the title of the task to plot\n",
    "    :instances: (int) the number of instances to average\n",
    "    \"\"\"\n",
    "    x, y = ts2xy(load_results(log_folder+'/env_0'), 'timesteps')\n",
    "\n",
    "    if instances > 1:\n",
    "        for i in range(1,instances):\n",
    "            _, y_tmp = ts2xy(load_results(log_folder+'/env_'+str(i)), 'timesteps')\n",
    "            if len(y) > len(y_tmp):\n",
    "                y = y[:len(y_tmp)] + y_tmp\n",
    "            else:\n",
    "                y = y + y_tmp[:len(y)]\n",
    "        y = y/instances\n",
    "    \n",
    "    y = moving_average(y, window=5) # change window value to change level of smoothness\n",
    "    # Truncate x\n",
    "    x = x[len(x) - len(y):]\n",
    "    fig = plt.figure(title)\n",
    "    plt.plot(x, y)\n",
    "    plt.xlabel('Number of Timesteps')\n",
    "    plt.ylabel('Rewards')\n",
    "    plt.title(title + \" Smoothed\")\n",
    "    if same_plot is False:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(log_dir, instances=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Test Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(4,)\n",
      "Box(36,)\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "# ############################################################################# Some Test Code\n",
    "def own_policy(obs):\n",
    "    return np.array([0,1,0,1,0,1,0,1,0,0,1,0,1,0,1,0,1,0])\n",
    "\n",
    "env = L2RunEnvMod(visualize=True, integrator_accuracy = 5e-3, own_policy=own_policy, muscle_synergy=None)\n",
    "env.load_model()\n",
    "observation = env.reset()\n",
    "print(env.action_space)\n",
    "print(env.observation_space)\n",
    "print(env.time_limit)\n",
    "\n",
    "# for i in range(200):\n",
    "#     # observation, reward, done, info = env.step(env.action_space.sample())\n",
    "#     observation, reward, done, info = env.step(np.zeros(18))\n",
    "#     clear_output(wait=True)\n",
    "#     print(reward)\n",
    "    \n",
    "# del env\n",
    "\n",
    "############################################################################## Visualize learning rate schedule\n",
    "# x = 1-np.arange(0,1,0.0001)\n",
    "# lr_exp = 1e-2*(np.exp(2*(x-1)))\n",
    "# plt.plot(np.arange(0,1,0.0001),lr_exp,label='exponential decay')\n",
    "# plt.title('learning rate scheduler comparison')\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'learning_rate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-cf456b93affe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m model = PPO2(MlpPolicy, env, verbose=0,  \n\u001b[1;32m----> 2\u001b[1;33m              \u001b[0mnoptepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m              tensorboard_log=tensorboard_log_dir)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'learning_rate' is not defined"
     ]
    }
   ],
   "source": [
    "model = PPO2(MlpPolicy, env, verbose=0,  \n",
    "             noptepochs=4, n_steps=128, learning_rate=learning_rate, \n",
    "             tensorboard_log=tensorboard_log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'model/pi_fc0/w:0' shape=(36, 64) dtype=float32_ref>,\n",
       " <tf.Variable 'model/pi_fc0/b:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'model/vf_fc0/w:0' shape=(36, 64) dtype=float32_ref>,\n",
       " <tf.Variable 'model/vf_fc0/b:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'model/pi_fc1/w:0' shape=(64, 64) dtype=float32_ref>,\n",
       " <tf.Variable 'model/pi_fc1/b:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'model/vf_fc1/w:0' shape=(64, 64) dtype=float32_ref>,\n",
       " <tf.Variable 'model/vf_fc1/b:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'model/vf/w:0' shape=(64, 1) dtype=float32_ref>,\n",
       " <tf.Variable 'model/vf/b:0' shape=(1,) dtype=float32_ref>,\n",
       " <tf.Variable 'model/pi/w:0' shape=(64, 18) dtype=float32_ref>,\n",
       " <tf.Variable 'model/pi/b:0' shape=(18,) dtype=float32_ref>,\n",
       " <tf.Variable 'model/pi/logstd:0' shape=(1, 18) dtype=float32_ref>,\n",
       " <tf.Variable 'model/q/w:0' shape=(64, 18) dtype=float32_ref>,\n",
       " <tf.Variable 'model/q/b:0' shape=(18,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_parameter_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
