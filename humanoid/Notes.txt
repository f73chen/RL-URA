PPO hyperparameters
- n_epochs --> use the same data multiple times (2-4)
	- problem because PPO is an on-policy model
	- trade off between sampling rate and model stability
- if performance fluctuates back and forth, try changing:
	- number of epochs
	- learning rate

- value is only dependent on state (expectation of reward based on a sample of trajectories)
	- advantage: increased value from taking some action which takes you to a different next state
	- estimate V(st) can be a bias from the true value
	- don't really need to change it
- clipping --> upper bound for policy update amount
	- try combination of large learning rate and smaller clip range (stablilize)
	- ensures final loss doesn't go beyong certain level
- entropy --> do whatever you want, as long as you reach the final goal
	- higher entropy encourages exploration between initial and final state
	- higher entropy decision = more random
- max_grad_norm --> clips update for each parameter
- look up vf_coef
- look up use_sde (similar to prioritized replay)
	- strategic exploration
	- different from max entropy
- look up target_kl
	- measures difference between entropy of different random variable distributions
	- probably not important
	- adds additional computation


Learn parameters
- callback can log per environment (but tensorboard doesn't)
	- can calculate statistics between vec envs


- imitative learning (do this first)
- muscle synergy (activation patterns from lit. review)